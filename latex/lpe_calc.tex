\documentclass[11pt]{article}

\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage[margin=1in]{geometry}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Prb}{\mathbb{P}}

\title{Estimating $\Pr[X=1^n]$ in a Bernoulli--Mixture Model:\\
Sample Complexity via Multiplicative Chernoff Bounds}
\author{}
\date{}

\begin{document}
\maketitle

\section{Setup}

Let $\Pi$ be an \emph{unknown} prior distribution on $p\in[0,1]$.
A random binary string $X=(X_1,\dots,X_n)\in\{0,1\}^n$ is generated by
\[
p \sim \Pi,
\qquad
X_1,\dots,X_n \mid p \ \stackrel{\text{iid}}{\sim}\ \mathrm{Bernoulli}(p).
\]
The quantity of interest is
\[
\theta \;:=\; \Prb[X = 1^n],
\]
where $1^n$ denotes the all-ones string of length $n$.

Conditioning on $p$,
\[
\Prb[X=1^n \mid p] = p^n,
\]
so by the law of total probability,
\begin{equation}
\label{eq:theta-def}
\theta \;=\; \E_{p\sim \Pi}[p^n].
\end{equation}

\paragraph{Goal (multiplicative accuracy).}
Fix a relative-error tolerance $\varepsilon\in(0,1)$ and failure probability $\delta\in(0,1)$.
We want an estimator $\widehat{\theta}$ such that
\[
\Prb\!\left[\,|\widehat{\theta}-\theta| \le \varepsilon \theta\,\right] \;\ge\; 1-\delta.
\]
Equivalently, the estimate is within a constant factor:
if $\varepsilon\in(0,1)$, then
\[
(1-\varepsilon)\theta \le \widehat{\theta} \le (1+\varepsilon)\theta.
\]

\section{A multiplicative Chernoff bound}

We will use a standard multiplicative Chernoff bound for sums of independent random variables in $[0,1]$.

\begin{lemma}[Multiplicative Chernoff for $[0,1]$ variables]
\label{lem:chernoff}
Let $W_1,\dots,W_m$ be independent random variables with $W_i\in[0,1]$.
Let $S=\sum_{i=1}^m W_i$ and $\mu=\E[S]$.
Then, for all $\varepsilon>0$,
\begin{align}
\Prb\!\left[S \ge (1+\varepsilon)\mu\right]
&\le \left(\frac{e^{\varepsilon}}{(1+\varepsilon)^{1+\varepsilon}}\right)^{\mu}.
\label{eq:chernoff-upper-exact}
\end{align}
For $\varepsilon\in(0,1)$, one has the convenient relaxations
\begin{align}
\Prb\!\left[S \ge (1+\varepsilon)\mu\right]
&\le \exp\!\left(-\frac{\mu \varepsilon^2}{3}\right),
\label{eq:chernoff-upper}
\\
\Prb\!\left[S \le (1-\varepsilon)\mu\right]
&\le \exp\!\left(-\frac{\mu \varepsilon^2}{2}\right)
\;\le\;
\exp\!\left(-\frac{\mu \varepsilon^2}{3}\right).
\label{eq:chernoff-lower}
\end{align}
\end{lemma}

\section{Scenario 1: only samples of $n$-bit strings}

Assume we can draw independent strings $X^{(1)},\dots,X^{(m)}$ from the marginal distribution on $\{0,1\}^n$ (with $p$ integrated out).
Define the indicator
\[
Y_i \;:=\; \mathbf{1}\{X^{(i)} = 1^n\}
\qquad (i=1,\dots,m).
\]
Then $Y_i\in\{0,1\}\subset[0,1]$ and
\[
\E[Y_i] = \Prb[X=1^n] = \theta,
\qquad
\text{so}\quad
\E\!\left[\sum_{i=1}^m Y_i\right] = m\theta.
\]
Consider the natural estimator
\[
\widehat{\theta}_1 \;:=\; \frac{1}{m}\sum_{i=1}^m Y_i.
\]

\begin{theorem}[Sample complexity in Scenario 1]
\label{thm:scenario1}
Fix $\varepsilon\in(0,1)$ and $\delta\in(0,1)$.
If
\begin{equation}
\label{eq:m1}
m \;\ge\; \frac{3}{\theta\,\varepsilon^2}\,\ln\!\frac{2}{\delta},
\end{equation}
then
\[
\Prb\!\left[\,|\widehat{\theta}_1-\theta| \le \varepsilon\theta\,\right] \;\ge\; 1-\delta.
\]
\end{theorem}

\begin{proof}
Let $S=\sum_{i=1}^m Y_i$, so $\mu=\E[S]=m\theta$ and $\widehat{\theta}_1=S/m$.
Applying Lemma~\ref{lem:chernoff} with $\varepsilon\in(0,1)$ gives
\[
\Prb\!\left[\widehat{\theta}_1 \ge (1+\varepsilon)\theta\right]
= \Prb\!\left[S \ge (1+\varepsilon)\mu\right]
\le \exp\!\left(-\frac{\mu\varepsilon^2}{3}\right)
= \exp\!\left(-\frac{m\theta\varepsilon^2}{3}\right),
\]
and similarly
\[
\Prb\!\left[\widehat{\theta}_1 \le (1-\varepsilon)\theta\right]
\le \exp\!\left(-\frac{m\theta\varepsilon^2}{3}\right).
\]
By a union bound,
\[
\Prb\!\left[\,|\widehat{\theta}_1-\theta| \ge \varepsilon\theta\,\right]
\le 2\exp\!\left(-\frac{m\theta\varepsilon^2}{3}\right).
\]
Requiring the right-hand side to be at most $\delta$ yields~\eqref{eq:m1}.
\end{proof}

\paragraph{Interpretation.}
The key dependence is
\[
m_1 \asymp \frac{1}{\theta}.
\]
When $\theta=\Prb[X=1^n]$ is very small, Scenario 1 requires very large $m$.

\subsection*{(Optional) Factor-$c$ form}
Sometimes one wants $\theta/c \le \widehat{\theta} \le c\theta$ for a constant $c>1$.
Let $S=\sum_{i=1}^m Y_i$ and $\mu=m\theta$.
Using the exact upper-tail form~\eqref{eq:chernoff-upper-exact} with $(1+\varepsilon)=c$,
\[
\Prb[S \ge c\mu]
\le \left(\frac{e^{c-1}}{c^c}\right)^{\mu}
= \exp\!\left(-\mu\big(c\ln c-(c-1)\big)\right).
\]
For the lower tail, set $\varepsilon_- = 1-\tfrac{1}{c}\in(0,1)$ and use~\eqref{eq:chernoff-lower}:
\[
\Prb\!\left[S \le \frac{1}{c}\mu\right]
= \Prb[S \le (1-\varepsilon_-)\mu]
\le \exp\!\left(-\frac{\mu\varepsilon_-^2}{2}\right).
\]
Thus it suffices that each tail is at most $\delta/2$, i.e.,
\[
m \;\ge\; 
\max\!\left\{
\frac{\ln(2/\delta)}{\theta\,\big(c\ln c-(c-1)\big)}\;,\;
\frac{2\ln(2/\delta)}{\theta\,\big(1-\frac{1}{c}\big)^2}
\right\}.
\]

\section{Scenario 2: direct samples from the prior over $p$}

Assume we can draw independent samples $p_1,\dots,p_m \sim \Pi$.
Define
\[
Z_i \;:=\; p_i^n.
\]
Then $Z_i\in[0,1]$ and, by~\eqref{eq:theta-def},
\[
\E[Z_i] = \E[p^n] = \theta.
\]
Consider the estimator
\[
\widehat{\theta}_2 \;:=\; \frac{1}{m}\sum_{i=1}^m Z_i.
\]

\subsection{A ``range-aware'' multiplicative Chernoff bound}

If we know (or can guarantee) an almost sure upper bound
\[
Z_i \le B \quad \text{for all }i
\qquad\text{where}\quad 0 < B \le 1,
\]
then we can scale $Z_i' := Z_i/B \in [0,1]$.
Let $S'=\sum_{i=1}^m Z_i'$ and $\mu'=\E[S']$.
Since $\E[Z_i']=\theta/B$, we have $\mu'=m\theta/B$.
Applying Lemma~\ref{lem:chernoff} to $S'$ yields the following.

\begin{theorem}[Sample complexity in Scenario 2 given $Z \le B$]
\label{thm:scenario2}
Fix $\varepsilon\in(0,1)$ and $\delta\in(0,1)$.
Assume $p^n \le B$ almost surely for some known $B\in(0,1]$.
If
\begin{equation}
\label{eq:m2}
m \;\ge\; \frac{3B}{\theta\,\varepsilon^2}\,\ln\!\frac{2}{\delta},
\end{equation}
then
\[
\Prb\!\left[\,|\widehat{\theta}_2-\theta| \le \varepsilon\theta\,\right] \;\ge\; 1-\delta.
\]
\end{theorem}

\begin{proof}
Let $S=\sum_{i=1}^m Z_i$ and define $S' := S/B = \sum_{i=1}^m (Z_i/B)$.
Then $Z_i/B\in[0,1]$, $S'\in[0,m]$, and
\[
\E[S'] = \frac{1}{B}\E[S] = \frac{1}{B}\cdot m\theta = \frac{m\theta}{B}.
\]
Also, $\widehat{\theta}_2 = S/m$ and $S' = (m/B)\widehat{\theta}_2$.
Thus,
\[
\widehat{\theta}_2 \ge (1+\varepsilon)\theta
\quad\Longleftrightarrow\quad
S' \ge (1+\varepsilon)\E[S'].
\]
Applying Lemma~\ref{lem:chernoff} with $\mu'=\E[S']=m\theta/B$ gives
\[
\Prb\!\left[\widehat{\theta}_2 \ge (1+\varepsilon)\theta\right]
\le \exp\!\left(-\frac{\mu'\varepsilon^2}{3}\right)
= \exp\!\left(-\frac{m\theta\varepsilon^2}{3B}\right),
\]
and similarly for the lower tail. Union bound and solving for $m$ yields~\eqref{eq:m2}.
\end{proof}

\paragraph{Where does $B$ come from?}
A common sufficient condition is an almost sure bound $p \le p_{\max}<1$, which implies
\[
p^n \le p_{\max}^n \quad\text{a.s.}
\qquad\Rightarrow\qquad
B = p_{\max}^n.
\]

\subsection*{(Optional) Factor-$c$ form}
Under the same assumption $p^n\le B$ a.s., it suffices that
\[
m \;\ge\;
\max\!\left\{
\frac{B\,\ln(2/\delta)}{\theta\,\big(c\ln c-(c-1)\big)}\;,\;
\frac{2B\,\ln(2/\delta)}{\theta\,\big(1-\frac{1}{c}\big)^2}
\right\}.
\]

\section{Comparison: when Scenario 2 is dramatically better}

Comparing Theorems~\ref{thm:scenario1} and~\ref{thm:scenario2} (relative error form),
\[
m_1 \;\gtrsim\; \frac{1}{\theta}
\qquad\text{whereas}\qquad
m_2 \;\gtrsim\; \frac{B}{\theta}.
\]
Thus, whenever $B\ll 1$, Scenario 2 reduces sample complexity by a factor of about $B$.

In particular, if $p \le p_{\max}<1$ almost surely, then $B=p_{\max}^n$ decays exponentially in $n$,
and for $n\approx 100$ this can be extremely small.

\paragraph{Special case.}
If one only knows $p\in[0,1]$ with no stronger bound, then $B=1$ and the two Chernoff-based bounds
match in their dependence on $(\theta,\varepsilon,\delta)$:
\[
m_2 \;\ge\; \frac{3}{\theta\,\varepsilon^2}\ln\frac{2}{\delta}
\quad\text{(same as Scenario 1).}
\]

\end{document}