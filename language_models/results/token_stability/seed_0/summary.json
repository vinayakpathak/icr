{
  "converged": true,
  "convergence_checkpoint": 280000,
  "effective_context_window": 256,
  "final_tokens": 280000,
  "final_unigram": [
    4.642857142857143e-05,
    0.0,
    0.07471428571428572,
    0.03748928571428571,
    0.007710714285714286,
    0.034307142857142855,
    0.007060714285714286,
    0.01817857142857143,
    0.033107142857142856,
    0.024139285714285714,
    0.009328571428571429,
    0.015092857142857142,
    0.030317857142857143,
    0.0197,
    0.0062107142857142856,
    0.0070464285714285715,
    0.03994285714285714,
    0.0025464285714285714,
    0.031078571428571428,
    0.013882142857142857,
    0.025775,
    0.008646428571428572,
    0.128775,
    0.011335714285714286,
    0.019592857142857144,
    0.006817857142857143,
    0.04391071428571429,
    0.012235714285714286,
    0.03738214285714286,
    0.03324642857142857,
    0.006303571428571428,
    0.024935714285714287,
    0.014417857142857144,
    0.06143571428571429,
    0.060067857142857145,
    0.035385714285714284,
    0.004675,
    0.0030535714285714285,
    0.01902857142857143,
    0.0007178571428571428,
    0.003014285714285714,
    0.014939285714285714,
    0.0062,
    0.003971428571428571,
    0.0019357142857142856,
    0.0001,
    9.642857142857143e-05,
    7.5e-05,
    7.142857142857143e-06,
    1.785714285714286e-05,
    3.5714285714285714e-06
  ],
  "max_order": 3,
  "max_position_embeddings": 256,
  "model_id": "phonemetransformers/GPT2-85M-CHAR-PHON",
  "runtime_sec": 5143.0784232616425,
  "seed": 0,
  "start_token_id": 3,
  "start_token_mode": "bos",
  "stop_reason": "converged",
  "supports": {
    "bigram_min_support": 500,
    "trigram_min_support": 200
  },
  "thresholds": {
    "bigram_tv_p95": 0.03,
    "trigram_tv_p95": 0.05,
    "unigram_max_delta": 0.002,
    "window_checkpoints": 5
  },
  "vocab_size": 51
}