{
  "converged": true,
  "convergence_checkpoint": 280000,
  "effective_context_window": 256,
  "final_tokens": 280000,
  "final_unigram": [
    0.00014642857142857144,
    0.0,
    0.04367857142857143,
    0.015085714285714286,
    0.008921428571428571,
    0.03887142857142857,
    0.014075,
    0.021435714285714287,
    0.042182142857142856,
    0.0231,
    0.014882142857142858,
    0.015428571428571429,
    0.03239285714285714,
    0.018217857142857143,
    0.006207142857142857,
    0.012146428571428572,
    0.06957142857142858,
    0.0029464285714285716,
    0.030335714285714285,
    0.0486,
    0.032592857142857146,
    0.014139285714285714,
    0.074,
    0.01637857142857143,
    0.013460714285714286,
    0.005614285714285714,
    0.02653214285714286,
    0.0216,
    0.07815714285714286,
    0.03246071428571429,
    0.015907142857142856,
    0.026939285714285714,
    0.013382142857142856,
    0.037032142857142854,
    0.03131428571428571,
    0.022442857142857143,
    0.004357142857142857,
    0.0047,
    0.026564285714285714,
    0.0021035714285714286,
    0.007667857142857143,
    0.023392857142857142,
    0.004907142857142857,
    0.0036357142857142855,
    0.0018392857142857143,
    0.00014285714285714287,
    0.00020357142857142858,
    0.0002642857142857143,
    3.5714285714285714e-06,
    3.2142857142857144e-05,
    7.142857142857143e-06
  ],
  "max_order": 3,
  "max_position_embeddings": 256,
  "model_id": "phonemetransformers/GPT2-85M-CHAR-PHON",
  "runtime_sec": 4731.425748825073,
  "seed": 4,
  "start_token_id": 3,
  "start_token_mode": "bos",
  "stop_reason": "converged",
  "supports": {
    "bigram_min_support": 500,
    "trigram_min_support": 200
  },
  "thresholds": {
    "bigram_tv_p95": 0.03,
    "trigram_tv_p95": 0.05,
    "unigram_max_delta": 0.002,
    "window_checkpoints": 5
  },
  "vocab_size": 51
}