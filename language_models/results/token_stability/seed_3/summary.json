{
  "converged": true,
  "convergence_checkpoint": 280000,
  "effective_context_window": 256,
  "final_tokens": 280000,
  "final_unigram": [
    0.00015357142857142856,
    0.0,
    0.07560357142857142,
    0.04878214285714286,
    0.010064285714285715,
    0.03533928571428571,
    0.014960714285714286,
    0.022510714285714287,
    0.05338214285714286,
    0.028753571428571427,
    0.014660714285714286,
    0.023578571428571428,
    0.04206785714285714,
    0.025464285714285714,
    0.011367857142857143,
    0.005646428571428571,
    0.048592857142857146,
    0.0026785714285714286,
    0.022382142857142857,
    0.028289285714285715,
    0.03352857142857143,
    0.010071428571428571,
    0.03394642857142857,
    0.02363214285714286,
    0.027546428571428572,
    0.007160714285714286,
    0.024067857142857144,
    0.009217857142857142,
    0.066225,
    0.03579642857142857,
    0.009532142857142857,
    0.025785714285714287,
    0.019678571428571427,
    0.025942857142857143,
    0.027342857142857144,
    0.02909642857142857,
    0.00525,
    0.003928571428571429,
    0.01747857142857143,
    0.0008285714285714286,
    0.002675,
    0.03356428571428571,
    0.005414285714285714,
    0.0032964285714285712,
    0.0029821428571428573,
    0.00022142857142857142,
    0.0006357142857142857,
    0.0008107142857142857,
    3.5714285714285714e-06,
    5.357142857142857e-05,
    7.142857142857143e-06
  ],
  "max_order": 3,
  "max_position_embeddings": 256,
  "model_id": "phonemetransformers/GPT2-85M-CHAR-PHON",
  "runtime_sec": 5130.845034360886,
  "seed": 3,
  "start_token_id": 3,
  "start_token_mode": "bos",
  "stop_reason": "converged",
  "supports": {
    "bigram_min_support": 500,
    "trigram_min_support": 200
  },
  "thresholds": {
    "bigram_tv_p95": 0.03,
    "trigram_tv_p95": 0.05,
    "unigram_max_delta": 0.002,
    "window_checkpoints": 5
  },
  "vocab_size": 51
}