{
  "converged": true,
  "convergence_checkpoint": 280000,
  "effective_context_window": 256,
  "final_tokens": 280000,
  "final_unigram": [
    0.00024642857142857143,
    0.0,
    0.11257857142857143,
    0.04776428571428572,
    0.013532142857142857,
    0.032467857142857146,
    0.009932142857142856,
    0.01752142857142857,
    0.04087857142857143,
    0.04935,
    0.01247857142857143,
    0.010332142857142857,
    0.02964642857142857,
    0.038714285714285715,
    0.004696428571428571,
    0.008810714285714286,
    0.044939285714285716,
    0.0024035714285714285,
    0.02395,
    0.019921428571428573,
    0.030089285714285714,
    0.008167857142857143,
    0.03876071428571429,
    0.016864285714285714,
    0.005014285714285714,
    0.004064285714285714,
    0.043832142857142854,
    0.01185,
    0.044196428571428574,
    0.040935714285714284,
    0.012417857142857144,
    0.03356428571428571,
    0.013889285714285715,
    0.033185714285714284,
    0.02964642857142857,
    0.016714285714285713,
    0.016264285714285714,
    0.007232142857142857,
    0.02915,
    0.0016178571428571429,
    0.0024964285714285713,
    0.02738214285714286,
    0.004689285714285714,
    0.0030785714285714288,
    0.0033107142857142858,
    0.0004,
    0.0006392857142857143,
    0.0002892857142857143,
    1.0714285714285714e-05,
    7.142857142857143e-05,
    1.0714285714285714e-05
  ],
  "max_order": 3,
  "max_position_embeddings": 256,
  "model_id": "phonemetransformers/GPT2-85M-CHAR-PHON",
  "runtime_sec": 21404.300640821457,
  "seed": 1,
  "start_token_id": 3,
  "start_token_mode": "bos",
  "stop_reason": "converged",
  "supports": {
    "bigram_min_support": 500,
    "trigram_min_support": 200
  },
  "thresholds": {
    "bigram_tv_p95": 0.03,
    "trigram_tv_p95": 0.05,
    "unigram_max_delta": 0.002,
    "window_checkpoints": 5
  },
  "vocab_size": 51
}